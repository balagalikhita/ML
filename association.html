<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="stylesheet" type="text/css" href="style1.css">
    <title>Association</title>
</head>

<body>
    <div class="main">
        <div class="menu">
            <h2 id="logo">Machine Learning</h2>
            <a href="#introassociat">Association</a>
            <a href="#apriori">Apriori</a>
            <a href="#Eclat">Eclat</a>
            <a href="#FP">F-P Growth</a>
            <a href="#associationappli">Applications of Association</a>
        </div>
        <div class="sidebody">
            <img src="images/logo.png" id="lo">
           <a href="index.html">Back to Home</a>
            <div id="title">Association</div>
            </br>
            <div id="introassociat">Association rule learning is a type of unsupervised learning technique that checks
                for the dependency of one data item on another data item and maps accordingly so that it can be more
                profitable. It tries to find some interesting relations or associations among the variables of dataset.
                It is based on different rules to discover the interesting relations between variables in the database.
            </div>
            <img style="position: absolute;
            left: 1125px;
            height: 264px;
            width: 350px;"src="/images/ass1.png" alt="">
            <div id="type"> Types of Association algorithms:
                <ol>
                    <li>1. Apriori</li>
                    <li>2. Eclat</li>
                    <li>3. F-P Growth Algorithm</li>
                </ol>
            </div>
            </br>
            <div id="constraint"><h3>Association Rule Learning work</h3>
                Association rule learning works on the concept of If and Else Statement, such as if A then B.Here the If
                element is called antecedent, and then statement is called as Consequent. These types of relationships
                where we can find out some association or relation between two items is known as single cardinality. It
                is all about creating rules, and if the number of items increases, then cardinality also increases
                accordingly. So, to measure the associations between thousands of data items, there are several metrics.
                These metrics are given below:
                <ol>
                    <li>o Support</li>
                    <li>o Confidence</li>
                    <li>o Lift</li>
                </ol>
            </div>
            </br>
            <div id="support">
                <h3>Support</h3>
                Support is the frequency of A or how frequently an item appears in the dataset. It is defined as the
                fraction of the transaction T that contains the itemset X. If there are X datasets, then for
                transactions T, it can be written as:
                </br>
                <img src="images/supp.jpg">
            </div>
            </br>
            <div id="confidence">
                <h3>Confidence</h3>
                Confidence indicates how often the rule has been found to be true. Or how often the items X and Y occur
                together in the dataset when the occurrence of X is already given. It is the ratio of the transaction
                that contains X and Y to the number of records that contain X.
                </br>
                <img src="images/conf.jpg">
            </div>
            </br>
            <div id="lift">
                <h3>Lift</h3>
                It is the strength of any rule, which can be defined as below formula:
                It is the ratio of the observed support measure and expected support if X and Y are independent of each
                other. It has three possible values:
                <ol>
                    <li>o If Lift= 1: The probability of occurrence of antecedent and consequent is independent of each
                        other.</li>
                    <li>o Lift>1: It determines the degree to which the two itemsets are dependent to each other.</li>
                    <li>o Lift<1: It tells us that one item is a substitute for other items, which means one item has a
                            negative effect on another.</li>

                </ol>
                <img src="images/lift.jpg">
            </div>
            </br>

            <div id="apriori">
                <h3>Apriori Algorithm</h3>
                Data Set – Market_Basket optimisation.csv from kaggle
                This algorithm uses frequent datasets to generate association rules. It is designed to work on the
                databases that contain transactions. This algorithm uses a breadth-first search and Hash Tree to
                calculate the itemset efficiently.
                It is mainly used for market basket analysis and helps to understand the products that can be bought
                together. It can also be used in the healthcare field to find drug reactions for patients.
                <ol>
                    <li> 1. Scan the transaction data base to get the support of S each 1-
                        itemset, compare S with min_sup, and get a support of 1-itemsets, L1.</li>
                    <li>2. Use Lk-1 join Lk-1 to generate a set of candidate k-itemsets and
                        use Apriori property to prune the unfrequented k-itemsets from this
                        set.</li>
                    <li>3. Scan the transaction database to get the support S of each candidate</li>
                    k-itemset in the find set, compare S with min_support, and get a set of
                    frequent k-itemsets Lk-1
                    <li>4. If the candidate set is not null, go to step 2.</li>
                    <li>5. For each frequent itemset l, generate all nonempty subsets of 1</li>
                    <li>6. For every non empty subset s of 1, output the rule “s=>(1-s)” if
                        confidence C of the rule “s=>(1-s)” (=support s of 1/support S of s)’
                        min_conf</li>
                    <li>7. End</li>

                </ol>
            </div>
            <h2 class="code">CODE</h2>
            <div class="scroll-block">
               <p>!pip install apyori</p> 
<p>import numpy as np</p>
<p>import pandas as pd</p>
<p>from apyori import apriori</p>
<p>data=pd.read_csv('/content/Market_Basket_Optimisation.csv')</p>
<p>data</p>
<p># Convert the dataset to a list of lists</p>
<p>items = []</p>
<p>for i in range(0, len(data)):</p>
    <p>items.append([str(data.values[i, j]) for j in range(0, len(data.columns))])</p>
<p>items</p>
<p># Use the apriori algorithm to discover association rules</p>
<p>final_rule = apriori(items, min_support=0.01, min_confidence=0.1, min_lift=1.0, min_length=2)</p>
<p>final_results = list(final_rule)</p>

<p>for rule in final_results[:10]:</p>
  <p>print(rule)</p>  

              </div>

            </br>
            <h2 class="code">OUTPUT</h2>
        <div class="scroll-block">
           <p>RelationRecord(items=frozenset({'chocolate'}),support=0.16386666666666666,ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'chocolate'}),confidence=0.16386666666666666, lift=1.0)])</p> 
<p>RelationRecord(items=frozenset({'eggs'}),support=0.17973333333333333, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'eggs'}),confidence=0.17973333333333333,lift=1.0))</p>
<p>RelationRecord(items=frozenset({'frenchfries'}),support=0.17093333333333333,ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'frenchfries'}),confidence=0.17093333333333333, lift=1.0)])</p>
<p>RelationRecord(items=frozenset({'greentea'}),support=0.132,ordered_statistics=[OrderedStatistic(items_base=frozenset(),items_add=frozenset({'greentea'}), confidence=0.132, lift=1.0)])</p>
<p>RelationRecord(items=frozenset({'milk'}),support=0.1296,ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'milk'}), confidence=0.1296, lift=1.0)])</p>
<p>RelationRecord(items=frozenset({'mineralwater'}),support=0.23826666666666665,ordered_statistics=[OrderedStatistic(items_base=frozenset(),items_add=frozenset({'mineral water'}), confidence=0.23826666666666665, lift=1.0)])</p>
<p>RelationRecord(items=frozenset({'nan'}),support=1.0,ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'nan'}), confidence=1.0, lift=1.0)])</p>
<p>RelationRecord(items=frozenset({'spaghetti'}),support=0.17413333333333333, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'spaghetti'}), confidence=0.17413333333333333, lift=1.0)])</p>
<p>RelationRecord(items=frozenset({'almonds', 'nan'}), support=0.020266666666666665, ordered_statistics=[OrderedStatistic(items_base=frozenset({'almonds'}), items_add=frozenset({'nan'}), confidence=1.0, lift=1.0)])</p>
<p>RelationRecord(items=frozenset({'mineralwater','avocado'}), support=0.011466666666666667,ordered_statistics=[OrderedStatistic(items_base=frozenset({'avocado'}),items_add=frozenset({'mineralwater'}),confidence=0.3453815261044177, lift=1.449558727354859)])</p>

        </div>
            <div id="Eclat">
                <h3>Eclat Algorithm</h3>
                Eclat algorithm stands for Equivalence Class Transformation. This algorithm uses a depth-first search
                technique to find frequent itemsets in a transaction database. It performs faster execution than Apriori
                Algorithm.
                <ol>
                    <li>1: Convert Transaction Data to Vertical Format</li>
                    <li>2: Candidate Generation From the Dataset</li>
                    <li>3: Pruning the Candidate Itemsets</li>
                    <li>4: Frequent Itemset Generation</li>
                    <li>5: Association Rule Generation</li>
                </ol>
            </div>
        </br>
        <h2 class="code">CODE</h2>
        <div class="scroll-block">
           <p> import pandas as pd</p>
<p>from mlxtend.preprocessing import TransactionEncoder</p>
<p>from mlxtend.frequent_patterns import apriori</p>

<p># Read the dataset</p>
<p>data = pd.read_csv('/content/Market_Basket_Optimisation.csv')</p>
<P># Convert the dataset to a list of lists</P>
<p>transactions = []</p>
<p>for i in range(len(data)):</p>
    <p>transactions.append([str(data.values[i, j]) for j in range(len(data.columns))])</p>

<p>te = TransactionEncoder()</p>
<P>te_ary = te.fit(transactions).transform(transactions)</P>
<p>df = pd.DataFrame(te_ary, columns=te.columns_)</p>

<p># Use the mlxtend.apriori function to apply the Eclat algorithm</p>
<p>frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)</p>

<p># Display the frequent itemsets</p>
<p>print(frequent_itemsets)</p>

        </div>
        <h2 class="code">OUTPUT</h2>
        <div class="scroll-block">
           <p>&ensp;&ensp; support   &ensp; &ensp;          itemsets</p> 
<p> 0]   0.163867           (chocolate)</p>
<p> 1]   0.179733                (eggs)</p>
<p> 2]   0.170933        (french fries)</p>
<p> 3]   0.132000           (green tea)</p>
<p> 4]   0.129600                (milk)</p>
<p> 5]   0.238267       (mineral water)</p>
<p> 6]   1.000000                 (nan)</p>
<p> 7]   0.174133           (spaghetti)</p>
<P> 8]   0.163867      (chocolate, nan)</P>
<p> 9]   0.179733           (eggs, nan)</p>
<p>10]  0.170933   (french fries, nan)</p>
<P>11]  0.132000      (green tea, nan)</P>
<P>12]  0.129600           (milk, nan)</P>
<p>13]  0.238267  (mineral water, nan)</p>
<P>14]  0.174133      (spaghetti, nan)</P>

        </div>
            <div id="FP">
                <h3>F-P Growth Algorithm</h3>
                The F-P growth algorithm stands for Frequent Pattern, and it is the improved version of the Apriori
                Algorithm. It represents the database in the form of a tree structure that is known as a frequent
                pattern or tree. The purpose of this frequent tree is to extract the most frequent patterns
            </br>
                <image class="img-fp" src="images/fp.jpg"/>
            </div>
        </br>
        <h2 class="code">CODE</h2>
        <div class="scroll-block">
            <p> from mlxtend.preprocessing import TransactionEncoder</p> 
          <p>from mlxtend.frequent_patterns import fpgrowth</p>
          <p>import pandas as pd</p>
          
          <p># Load the Market Basket Optimization dataset</p>
          <P>data = pd.read_csv('/content/Market_Basket_Optimisation.csv', header=None)</P>
          
          <p># Convert the dataset to a list of lists</p>
          <p>transactions = []</p>
          <p>for i in range(len(data)):</p>
             <p> &ensp;transactions.append([str(data.values[i, j]) for j in range(len(data.columns))])</p>
          
          <p># Use TransactionEncoder to one-hot encode the transactions</p>
          <p>te = TransactionEncoder()</p>
          <p>te_ary = te.fit(transactions).transform(transactions)</p>
          <P>df = pd.DataFrame(te_ary, columns=te.columns_)</P>
          
          <p># Apply the FP-growth algorithm</p>
          <p>frequent_itemsets = fpgrowth(df, min_support=0.05, use_colnames=True)</p>
          
          <p># Display the frequent itemsets</p>
          <p>print("Frequent Itemsets:")</p>
          <p>print(frequent_itemsets.head())</p>
          
          </div>
          <h2 class="code">OUTPUT</h2>
          <div class="scroll-block">
            Frequent Itemsets:
  <p>&ensp; &ensp;support    &ensp; &ensp;      itemsets</p>  
<p>0]  0.238368   (mineral water)</p>
<p>1]  0.132116       (green tea)</p>
<p>2]  0.076523  (low fat yogurt)</p>
<p>3]  0.071457          (shrimp)</p>
<p>4]  0.065858       (olive oil)</p>

          </div>

            <div id="associationappli">
                <h3>Applications of Association Rule Learning</h3>
                It has various applications in machine learning and data mining. Below are some popular applications of
                association rule learning:
                <li>o Market Basket Analysis: It is one of the popular examples and applications of association rule
                    mining. This technique is commonly used by big retailers to determine the association between items.
                </li>
                <li>o Medical Diagnosis: With the help of association rules, patients can be cured easily, as it helps
                    in identifying the probability of illness for a particular disease.</li>
                <li>o Protein Sequence: The association rules help in determining the synthesis of artificial Proteins.
                </li>
                <li>o It is also used for the Catalog Design and Loss-leader Analysis and many more other applications
                </li>

            </div>

        </div>
</body>

</html>